# Manifold Learning & Optimization
## Optimization Algorithms
Gradient Descent(GD) is the most fundamental method for training NN. 
If the hyperparameter also known as the learning rate is given, 
NN adjust it's weights automatically by using the chain rule. 
Other algorhithms such as momentum, Adagrad, Adam are all algorithms based on GD.


## Manifold Learning
### Isomap
### Locally Linear Embedding
### Laplacian Eigenmaps
